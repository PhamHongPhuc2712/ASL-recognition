{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# mediapipe for hand tracking\n",
        "import mediapipe as mp\n",
        "\n",
        "# gpu or cpu\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(\"=\" * 30)\n",
        "print(f\"Device in use: {device}\")\n",
        "print(\"Setup Complete.\")\n",
        "print(\"=\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive\"\n",
        "DATA_DIR = f\"{BASE_DIR}/data\"\n",
        "ASL_DIR = f\"{DATA_DIR}/asl_alphabet\"\n",
        "\n",
        "# create directories\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(ASL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✔ Created directories:\")\n",
        "print(DATA_DIR)\n",
        "print(ASL_DIR)\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\" HOW TO ATTACH DATASETS TO THIS NOTEBOOK\")\n",
        "print(\"============================================================\")\n",
        "print(\"\"\"\n",
        "STEP 1 — Click 'Add Data' on the right sidebar.\n",
        "\n",
        "STEP 2 — Search & attach the following datasets:\n",
        "\n",
        "    ASL Alphabet Dataset (Grassknoted)\n",
        "    https://www.kaggle.com/datasets/grassknoted/asl-alphabet\n",
        "\n",
        "STEP 3 — After attaching all datasets, run the next cell to verify paths.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_path(possible_paths):\n",
        "    for p in possible_paths:\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "# ASL Alphabet dataset\n",
        "asl_path = find_path([\n",
        "    # Google Colab paths\n",
        "    \"/content/drive/MyDrive/data/asl_alphabet\",\n",
        "    \"/content/drive/MyDrive/data/asl_alphabet_train\",\n",
        "    \"/content/asl_alphabet\",\n",
        "])\n",
        "\n",
        "print(\"============================================================\")\n",
        "print(\" DATASET PATH CHECK RESULTS\")\n",
        "print(\"============================================================\")\n",
        "print(f\"ASL Alphabet found: {asl_path}\")\n",
        "\n",
        "if asl_path: print(\"✔ ASL Alphabet correctly attached.\")\n",
        "else: print(\"❌ ASL Alphabet NOT FOUND — attach using 'Add Data'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract 3D Landmarks from ASL Images\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "\n",
        "DATA_DIR = \"data/asl_alphabet\"  u# pdate to dataset path\n",
        "OUTPUT_DIR = \"output\"\n",
        "MAX_IMAGES_PER_CLASS = 1000\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "# Get class folders (subdirectories in DATA_DIR)\n",
        "class_folders = sorted([f for f in os.listdir(DATA_DIR) \n",
        "                        if os.path.isdir(os.path.join(DATA_DIR, f))])\n",
        "print(f\"Found {len(class_folders)} classes: {class_folders}\")\n",
        "\n",
        "# Create label map\n",
        "label_map = {folder: idx for idx, folder in enumerate(class_folders)}\n",
        "print(f\"Label map: {label_map}\")\n",
        "\n",
        "# Storage\n",
        "landmarks_list = []\n",
        "labels_list = []\n",
        "no_hand_count = 0\n",
        "\n",
        "# Process each class\n",
        "for class_name in tqdm(class_folders, desc=\"Processing classes\"):\n",
        "    class_path = os.path.join(DATA_DIR, class_name)\n",
        "    images = [f for f in os.listdir(class_path) \n",
        "              if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    \n",
        "    # Limit to MAX_IMAGES_PER_CLASS\n",
        "    images = images[:MAX_IMAGES_PER_CLASS]\n",
        "    \n",
        "    print(f\"Processing class '{class_name}': {len(images)} images\")\n",
        "    \n",
        "    for img_name in images:\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "        \n",
        "        # Read and convert image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Process with MediaPipe\n",
        "        results = hands.process(img_rgb)\n",
        "        \n",
        "        if results.multi_hand_landmarks:\n",
        "            # Extract 21 landmarks (x, y, z for each = 63 values)\n",
        "            hand_landmarks = results.multi_hand_landmarks[0]\n",
        "            landmark_values = []\n",
        "            for landmark in hand_landmarks.landmark:\n",
        "                landmark_values.extend([landmark.x, landmark.y, landmark.z])\n",
        "            \n",
        "            landmarks_list.append(landmark_values)\n",
        "            labels_list.append(label_map[class_name])\n",
        "        else:\n",
        "            no_hand_count += 1\n",
        "            print(f\"  No hand detected: {img_name}\")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "landmarks_array = np.array(landmarks_list, dtype=np.float32)\n",
        "labels_array = np.array(labels_list, dtype=np.int32)\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"\\n=== Results ===\")\n",
        "print(f\"Landmarks shape: {landmarks_array.shape} (expected: (N, 63))\")\n",
        "print(f\"Labels shape: {labels_array.shape} (expected: (N,))\")\n",
        "print(f\"Images with no hand detected: {no_hand_count}\")\n",
        "\n",
        "# Save outputs\n",
        "np.save(os.path.join(OUTPUT_DIR, \"landmarks.npy\"), landmarks_array)\n",
        "np.save(os.path.join(OUTPUT_DIR, \"labels.npy\"), labels_array)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, \"label_map.json\"), \"w\") as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "\n",
        "print(f\"\\n✔ Saved to {OUTPUT_DIR}:\")\n",
        "print(f\"  - landmarks.npy\")\n",
        "print(f\"  - labels.npy\")\n",
        "print(f\"  - label_map.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize Skeletal Hand Data and Split into Train/Validation Sets\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Paths\n",
        "OUTPUT_DIR = \"output\"\n",
        "NORMALIZED_DIR = \"normalized\"\n",
        "os.makedirs(NORMALIZED_DIR, exist_ok=True)\n",
        "\n",
        "# Load data\n",
        "landmarks = np.load(os.path.join(OUTPUT_DIR, \"landmarks.npy\"))\n",
        "labels = np.load(os.path.join(OUTPUT_DIR, \"labels.npy\"))\n",
        "\n",
        "print(f\"Loaded landmarks shape: {landmarks.shape}\")\n",
        "print(f\"Loaded labels shape: {labels.shape}\")\n",
        "\n",
        "# Reshape from (N, 63) to (N, 21, 3)\n",
        "N = landmarks.shape[0]\n",
        "landmarks_reshaped = landmarks.reshape(N, 21, 3)\n",
        "print(f\"Reshaped landmarks: {landmarks_reshaped.shape}\")\n",
        "\n",
        "def normalize_hand(landmarks_21x3):\n",
        "    \"\"\"Center around wrist and scale by max distance from wrist.\"\"\"\n",
        "    # Center: subtract wrist (landmark 0) from all joints\n",
        "    wrist = landmarks_21x3[0]  # (3,)\n",
        "    centered = landmarks_21x3 - wrist  # (21, 3)\n",
        "    \n",
        "    # Scale: divide by max Euclidean distance from wrist to any other joint\n",
        "    distances = np.linalg.norm(centered[1:], axis=1)  # distances from wrist to other joints\n",
        "    max_distance = np.max(distances)\n",
        "    \n",
        "    if max_distance > 0:\n",
        "        scaled = centered / max_distance\n",
        "    else:\n",
        "        scaled = centered\n",
        "    \n",
        "    return scaled\n",
        "\n",
        "# Normalize all samples\n",
        "landmarks_normalized = np.array([normalize_hand(sample) for sample in landmarks_reshaped])\n",
        "print(f\"Normalized landmarks shape: {landmarks_normalized.shape}\")\n",
        "\n",
        "# Stratified split - 10% validation\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "train_idx, val_idx = next(sss.split(landmarks_normalized, labels))\n",
        "\n",
        "# Split data\n",
        "X_train = landmarks_normalized[train_idx]\n",
        "X_val = landmarks_normalized[val_idx]\n",
        "y_train = labels[train_idx]\n",
        "y_val = labels[val_idx]\n",
        "\n",
        "# Save outputs\n",
        "np.save(os.path.join(NORMALIZED_DIR, \"landmarks_normalized.npy\"), landmarks_normalized)\n",
        "np.save(os.path.join(NORMALIZED_DIR, \"labels.npy\"), labels)\n",
        "np.save(os.path.join(NORMALIZED_DIR, \"train_indices.npy\"), train_idx)\n",
        "np.save(os.path.join(NORMALIZED_DIR, \"val_indices.npy\"), val_idx)\n",
        "\n",
        "# Verification\n",
        "print(f\"\\n=== Results ===\")\n",
        "print(f\"Training set: {X_train.shape}, labels: {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, labels: {y_val.shape}\")\n",
        "print(f\"\\nSample normalized landmark (first sample, first 3 joints):\")\n",
        "print(landmarks_normalized[0][:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1D-CNN Model with Residual Connections for ASL Classification\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load normalized data\n",
        "NORMALIZED_DIR = \"normalized\"\n",
        "landmarks = np.load(os.path.join(NORMALIZED_DIR, \"landmarks_normalized.npy\"))\n",
        "labels = np.load(os.path.join(NORMALIZED_DIR, \"labels.npy\"))\n",
        "train_idx = np.load(os.path.join(NORMALIZED_DIR, \"train_indices.npy\"))\n",
        "val_idx = np.load(os.path.join(NORMALIZED_DIR, \"val_indices.npy\"))\n",
        "\n",
        "# Split data using saved indices\n",
        "X_train = landmarks[train_idx]\n",
        "X_val = landmarks[val_idx]\n",
        "y_train = labels[train_idx]\n",
        "y_val = labels[val_idx]\n",
        "\n",
        "num_classes = len(np.unique(labels))\n",
        "print(f\"Training: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Residual Block\n",
        "def residual_block(x, filters, kernel_size=3):\n",
        "    \"\"\"1D Residual block with skip connection.\"\"\"\n",
        "    shortcut = x\n",
        "    \n",
        "    # First conv\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    \n",
        "    # Second conv\n",
        "    x = layers.Conv1D(filters, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    # Skip connection: match dimensions if needed\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
        "    \n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "# Build Model\n",
        "inputs = layers.Input(shape=(21, 3))  # 21 joints, 3D coordinates\n",
        "\n",
        "# Initial Conv\n",
        "x = layers.Conv1D(64, 3, padding='same')(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation('relu')(x)\n",
        "\n",
        "# Residual blocks with increasing filters\n",
        "x = residual_block(x, 64)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = residual_block(x, 128)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "\n",
        "x = residual_block(x, 256)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Classification head\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save model\n",
        "model.save(\"asl_resnet_model.keras\")\n",
        "print(\"\\n✔ Model saved to asl_resnet_model.keras\")\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.plot(history.history['accuracy'], label='Train')\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation')\n",
        "ax1.set_title('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(history.history['loss'], label='Train')\n",
        "ax2.plot(history.history['val_loss'], label='Validation')\n",
        "ax2.set_title('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_history.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"✔ Training history saved to training_history.png\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
